{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a3f5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bloomberg xbbg connectivity successful\n",
      "\n",
      "=== INDIVIDUAL TICKER TESTING ===\n",
      "      Attempting to fetch UUP US Equity...\n",
      "      Attempting to fetch VUSTX US Equity...\n",
      "      Attempting to fetch SPY US Equity...\n",
      "      Attempting to fetch USGG30YR Index...\n",
      "      Attempting to fetch VFINX US Equity...\n",
      "      Attempting to fetch TLT US Equity...\n",
      "      Attempting to fetch DXY Curncy...\n",
      "      Attempting to fetch GLD US Equity...\n",
      "      Attempting to fetch US0003M Index...\n",
      "      Attempting to fetch SPX Index...\n",
      "      Attempting to fetch GOLDS Index...\n",
      "      Attempting to fetch CRB Index...\n",
      "      Attempting to fetch CRB Index...\n",
      "      Attempting to fetch CRB Index...\n",
      "      Failed to retrieve data for CRB Index after 3 attempts\n",
      "      Attempting to fetch DBC US Equity...\n",
      "      Attempting to fetch XAU Curncy...\n",
      "      Attempting to fetch USDINDEX Index...\n",
      "      Attempting to fetch USDINDEX Index...\n",
      "      Attempting to fetch USDINDEX Index...\n",
      "      Failed to retrieve data for USDINDEX Index after 3 attempts\n",
      "      Attempting to fetch BCOM Index...\n",
      "  UUP US Equity: ✓ 3 observations\n",
      "  VUSTX US Equity: ✓ 62 observations\n",
      "  SPY US Equity: ✓ 3 observations\n",
      "  USGG30YR Index: ✓ 3 observations\n",
      "  VFINX US Equity: ✓ 3 observations\n",
      "  TLT US Equity: ✓ 3 observations\n",
      "  DXY Curncy: ✓ 65 observations\n",
      "  GLD US Equity: ✓ 3 observations\n",
      "  US0003M Index: ✓ 3 observations\n",
      "  SPX Index: ✓ 3 observations\n",
      "  GOLDS Index: ✓ 65 observations\n",
      "  CRB Index: ✗ No data retrieved\n",
      "  DBC US Equity: ✓ 3 observations\n",
      "  XAU Curncy: ✓ 65 observations\n",
      "  USDINDEX Index: ✗ No data retrieved\n",
      "  BCOM Index: ✓ 3 observations\n",
      "\n",
      "=== FETCHING COMPLETE DATASET ===\n",
      "Fetching Defense First dataset from 1986-01-01 to 2025-06-30\n",
      "Processing TLT...\n",
      "    Fetching proxy data for TLT pre-2002-07-31...\n",
      "      Attempting to fetch VUSTX US Equity...\n",
      "    Fetching ETF data for TLT from 2002-07-31...\n",
      "      Attempting to fetch TLT US Equity...\n",
      "  ✓ TLT: 4376 observations from 1986-05-19 to 2025-06-30\n",
      "Processing GLD...\n",
      "    Fetching proxy data for GLD pre-2004-11-30...\n",
      "      Attempting to fetch XAU Curncy...\n",
      "    Fetching ETF data for GLD from 2004-11-30...\n",
      "      Attempting to fetch GLD US Equity...\n",
      "  ✓ GLD: 5098 observations from 1986-01-02 to 2025-06-30\n",
      "Processing DBC...\n",
      "    Fetching proxy data for DBC pre-2006-02-28...\n",
      "      Attempting to fetch BCOM Index...\n",
      "    Fetching ETF data for DBC from 2006-02-28...\n",
      "      Attempting to fetch DBC US Equity...\n",
      "  ✓ DBC: 474 observations from 1986-01-31 to 2025-06-30\n",
      "Processing UUP...\n",
      "    Fetching proxy data for UUP pre-2007-03-30...\n",
      "      Attempting to fetch DXY Curncy...\n",
      "    Fetching ETF data for UUP from 2007-03-30...\n",
      "      Attempting to fetch UUP US Equity...\n",
      "  ✓ UUP: 5666 observations from 1986-01-02 to 2025-06-30\n",
      "Processing SPY...\n",
      "    Fetching proxy data for SPY pre-1995-01-31...\n",
      "      Attempting to fetch SPX Index...\n",
      "    Fetching ETF data for SPY from 1995-01-31...\n",
      "      Attempting to fetch SPY US Equity...\n",
      "  ✓ SPY: 474 observations from 1986-01-31 to 2025-06-30\n",
      "Processing cash proxy (90-day T-bills)...\n",
      "      Attempting to fetch US0003M Index...\n",
      "  ✓ CASH: 465 observations\n",
      "\n",
      "=== DATA QUALITY REPORT ===\n",
      "\n",
      "✓ TLT:\n",
      "  Observations: 474\n",
      "  Coverage: 99.2% (470 valid)\n",
      "  Date Range: 1986-05 to 2025-06\n",
      "  Value Range: 64.40 to 323.98\n",
      "\n",
      "✓ GLD:\n",
      "  Observations: 474\n",
      "  Coverage: 100.0% (474 valid)\n",
      "  Date Range: 1986-01 to 2025-06\n",
      "  Value Range: 25.41 to 304.81\n",
      "\n",
      "✓ DBC:\n",
      "  Observations: 474\n",
      "  Coverage: 100.0% (474 valid)\n",
      "  Date Range: 1986-01 to 2025-06\n",
      "  Value Range: 8.86 to 47.35\n",
      "\n",
      "✓ UUP:\n",
      "  Observations: 474\n",
      "  Coverage: 100.0% (474 valid)\n",
      "  Date Range: 1986-01 to 2025-06\n",
      "  Value Range: 21.26 to 36.27\n",
      "\n",
      "✓ SPY:\n",
      "  Observations: 474\n",
      "  Coverage: 100.0% (474 valid)\n",
      "  Date Range: 1986-01 to 2025-06\n",
      "  Value Range: 16.14 to 1059.22\n",
      "\n",
      "✓ CASH:\n",
      "  Observations: 474\n",
      "  Coverage: 100.0% (474 valid)\n",
      "  Date Range: 1986-01 to 2025-06\n",
      "  Value Range: 100.67 to 402.68\n",
      "\n",
      "=== DATETIME INDEX VALIDATION ===\n",
      "✓ is_datetime_index: True\n",
      "✓ is_sorted: True\n",
      "✓ has_duplicates: False\n",
      "✓ frequency: M\n",
      "✗ timezone_aware: False\n",
      "✗ index_name: None\n",
      "\n",
      "Dataset shape: (474, 6)\n",
      "Date range: 1986-01-31 to 2025-06-30\n",
      "Frequency: M\n",
      "Assets with data: TLT, GLD, DBC, UUP, SPY, CASH\n",
      "Data saved to defense_first_data.csv (Date as datetime index)\n",
      "Data saved to defense_first_data.parquet (datetime index preserved)\n",
      "Data saved to defense_first_data.xlsx (datetime index with proper formatting)\n",
      "Data loader helper saved to defense_first_data_loader.py\n",
      "\n",
      "=== SAMPLE DATA (Last 5 observations) ===\n",
      "                 TLT     GLD      DBC      UUP        SPY        CASH\n",
      "2025-02-28  197.9784  263.34  26.8638  34.5201  1011.6216  402.676836\n",
      "2025-03-31  195.5901  288.17  27.4867  33.5449   955.6324  402.676836\n",
      "2025-04-30  192.9224  303.84  25.1295  32.2113   948.0272  402.676836\n",
      "2025-05-31  186.7311  303.61  25.5020  32.2524  1007.2457  402.676836\n",
      "2025-06-30  191.7113  304.81  26.6317  31.5710  1059.2203  402.676836\n",
      "\n",
      "=== FIRST VALID DATA POINTS ===\n",
      "TLT: 1986-05-31 = 74.32\n",
      "GLD: 1986-01-31 = 34.83\n",
      "DBC: 1986-01-31 = 10.01\n",
      "UUP: 1986-01-31 = 36.27\n",
      "SPY: 1986-01-31 = 16.14\n",
      "CASH: 1986-01-31 = 100.67\n",
      "\n",
      "✓ All files saved with proper datetime index formatting\n",
      "✓ Data loader helper created: defense_first_data_loader.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xbbg import blp\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DefenseFirstDataPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Defense First Strategy Data Pipeline - Fixed for xbbg MultiIndex issues\n",
    "        Handles Bloomberg data retrieval, proxy stitching, and data quality validation\n",
    "        \"\"\"\n",
    "        # Updated inception dates based on actual Bloomberg data availability\n",
    "        self.assets = {\n",
    "            'TLT': {\n",
    "                'bloomberg': 'TLT US Equity',\n",
    "                'inception_date': '2002-07-31',  # Updated to actual first data date\n",
    "                'proxy_pre_inception': 'VUSTX US Equity',  # Vanguard Long-Term Treasury\n",
    "                'proxy_alt': 'USGG30YR Index'  # 30-Year Treasury Yield (convert to price)\n",
    "            },\n",
    "            'GLD': {\n",
    "                'bloomberg': 'GLD US Equity', \n",
    "                'inception_date': '2004-11-30',  # Updated to actual first data date\n",
    "                'proxy_pre_inception': 'XAU Curncy',  # Gold Spot\n",
    "                'proxy_alt': 'GOLDS Index'  # Gold Index\n",
    "            },\n",
    "            'DBC': {\n",
    "                'bloomberg': 'DBC US Equity',\n",
    "                'inception_date': '2006-02-28',  # Updated to actual first data date\n",
    "                'proxy_pre_inception': 'BCOM Index',  # Bloomberg Commodity Index (switched to working proxy)\n",
    "                'proxy_alt': 'CRB Index'  # CRB Commodity Index (now alternative)\n",
    "            },\n",
    "            'UUP': {\n",
    "                'bloomberg': 'UUP US Equity',\n",
    "                'inception_date': '2007-03-30',  # Updated to actual first data date\n",
    "                'proxy_pre_inception': 'DXY Curncy',  # US Dollar Index\n",
    "                'proxy_alt': 'USDINDEX Index'\n",
    "            },\n",
    "            'SPY': {\n",
    "                'bloomberg': 'SPY US Equity',\n",
    "                'inception_date': '1995-01-31',  # Updated to actual first data date (SPY had limited early data)\n",
    "                'proxy_pre_inception': 'SPX Index',  # S&P 500 Index\n",
    "                'proxy_alt': 'VFINX US Equity'  # Vanguard 500 Fund\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.cash_proxy = 'US0003M Index'  # 90-day T-bills\n",
    "        self.cash_alt = 'FEDL01 Index'     # Federal Funds Rate\n",
    "        \n",
    "    def get_complete_dataset(self, start_date='1986-01-01', end_date=None, frequency='M'):\n",
    "        \"\"\"\n",
    "        Main method to retrieve complete historical dataset with proxy stitching\n",
    "        \"\"\"\n",
    "        if end_date is None:\n",
    "            end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "            \n",
    "        print(f\"Fetching Defense First dataset from {start_date} to {end_date}\")\n",
    "        \n",
    "        # Initialize results container\n",
    "        final_data = {}\n",
    "        \n",
    "        # Process each asset with proxy stitching\n",
    "        for asset_name, asset_config in self.assets.items():\n",
    "            print(f\"Processing {asset_name}...\")\n",
    "            \n",
    "            try:\n",
    "                asset_data = self._get_asset_complete_history(\n",
    "                    asset_name, asset_config, start_date, end_date, frequency\n",
    "                )\n",
    "                \n",
    "                if not asset_data.empty:\n",
    "                    final_data[asset_name] = asset_data\n",
    "                    print(f\"  ✓ {asset_name}: {len(asset_data)} observations from {asset_data.index[0]} to {asset_data.index[-1]}\")\n",
    "                else:\n",
    "                    print(f\"  ✗ {asset_name}: No data retrieved\")\n",
    "                    final_data[asset_name] = pd.Series()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error processing {asset_name}: {e}\")\n",
    "                final_data[asset_name] = pd.Series()\n",
    "        \n",
    "        # Get cash proxy data\n",
    "        print(\"Processing cash proxy (90-day T-bills)...\")\n",
    "        try:\n",
    "            cash_data = self._get_cash_data(start_date, end_date, frequency)\n",
    "            final_data['CASH'] = cash_data\n",
    "            if not cash_data.empty:\n",
    "                print(f\"  ✓ CASH: {len(cash_data)} observations\")\n",
    "            else:\n",
    "                print(f\"  ✗ CASH: No data retrieved\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error processing cash data: {e}\")\n",
    "            final_data['CASH'] = pd.Series()\n",
    "        \n",
    "        # Create date range for alignment\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "        \n",
    "        # Combine all data into single DataFrame with proper alignment\n",
    "        combined_data = pd.DataFrame(index=date_range)\n",
    "        for asset_name, asset_series in final_data.items():\n",
    "            if not asset_series.empty:\n",
    "                combined_data[asset_name] = asset_series.reindex(date_range, method='ffill')\n",
    "            else:\n",
    "                combined_data[asset_name] = np.nan\n",
    "        \n",
    "        # Data quality validation\n",
    "        quality_report = self._validate_data_quality(combined_data)\n",
    "        print(\"\\n=== DATA QUALITY REPORT ===\")\n",
    "        self._print_quality_report(quality_report)\n",
    "        \n",
    "        return combined_data\n",
    "    \n",
    "    def _get_asset_complete_history(self, asset_name, asset_config, start_date, end_date, frequency):\n",
    "        \"\"\"\n",
    "        Get complete history for single asset with proxy stitching and fallback logic\n",
    "        \"\"\"\n",
    "        inception_date = asset_config['inception_date']\n",
    "        bloomberg_ticker = asset_config['bloomberg']\n",
    "        proxy_ticker = asset_config['proxy_pre_inception']\n",
    "        proxy_alt = asset_config.get('proxy_alt', '')\n",
    "        \n",
    "        # Determine if we need proxy data\n",
    "        if pd.to_datetime(start_date) < pd.to_datetime(inception_date):\n",
    "            # Need proxy data for pre-inception period\n",
    "            print(f\"    Fetching proxy data for {asset_name} pre-{inception_date}...\")\n",
    "            \n",
    "            # Try primary proxy first\n",
    "            proxy_data = self._fetch_single_series(\n",
    "                proxy_ticker, start_date, inception_date, frequency\n",
    "            )\n",
    "            \n",
    "            # If primary proxy fails and alternative exists, try alternative\n",
    "            if proxy_data.empty and proxy_alt:\n",
    "                print(f\"    Primary proxy failed, trying alternative: {proxy_alt}...\")\n",
    "                proxy_data = self._fetch_single_series(\n",
    "                    proxy_alt, start_date, inception_date, frequency\n",
    "                )\n",
    "            \n",
    "            # Get ETF data from inception forward\n",
    "            print(f\"    Fetching ETF data for {asset_name} from {inception_date}...\")\n",
    "            etf_data = self._fetch_single_series(\n",
    "                bloomberg_ticker, inception_date, end_date, frequency\n",
    "            )\n",
    "            \n",
    "            # Stitch data together\n",
    "            if not proxy_data.empty and not etf_data.empty:\n",
    "                complete_data = self._stitch_proxy_to_etf(proxy_data, etf_data, inception_date)\n",
    "            elif not etf_data.empty:\n",
    "                complete_data = etf_data\n",
    "            elif not proxy_data.empty:\n",
    "                complete_data = proxy_data\n",
    "            else:\n",
    "                complete_data = pd.Series()\n",
    "            \n",
    "        else:\n",
    "            # Only need ETF data\n",
    "            complete_data = self._fetch_single_series(\n",
    "                bloomberg_ticker, start_date, end_date, frequency\n",
    "            )\n",
    "        \n",
    "        return complete_data\n",
    "    \n",
    "    def _fetch_single_series(self, ticker, start_date, end_date, frequency, retries=3):\n",
    "        \"\"\"\n",
    "        Fetch single Bloomberg series with proper xbbg handling\n",
    "        \"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                # Fetch data - handle both total return and price\n",
    "                print(f\"      Attempting to fetch {ticker}...\")\n",
    "                \n",
    "                # Try total return first\n",
    "                try:\n",
    "                    data = blp.bdh(\n",
    "                        tickers=[ticker],  # Use list to ensure consistent structure\n",
    "                        flds=['TOT_RETURN_INDEX_GROSS_DVDS'],\n",
    "                        start_date=start_date,\n",
    "                        end_date=end_date,\n",
    "                        Per=frequency,\n",
    "                        CshAdjNormal=True,\n",
    "                        CshAdjAbnormal=True,\n",
    "                        CapChg=True\n",
    "                    )\n",
    "                    \n",
    "                    if not data.empty and len(data.columns) > 0:\n",
    "                        # Handle MultiIndex columns properly\n",
    "                        if isinstance(data.columns, pd.MultiIndex):\n",
    "                            # Get first non-null column\n",
    "                            for col in data.columns:\n",
    "                                series = data[col].dropna()\n",
    "                                if not series.empty:\n",
    "                                    return series\n",
    "                        else:\n",
    "                            return data.iloc[:, 0].dropna()\n",
    "                            \n",
    "                except Exception:\n",
    "                    # Fall back to price data\n",
    "                    data = blp.bdh(\n",
    "                        tickers=[ticker],\n",
    "                        flds=['PX_LAST'],\n",
    "                        start_date=start_date,\n",
    "                        end_date=end_date,\n",
    "                        Per=frequency\n",
    "                    )\n",
    "                    \n",
    "                    if not data.empty and len(data.columns) > 0:\n",
    "                        if isinstance(data.columns, pd.MultiIndex):\n",
    "                            for col in data.columns:\n",
    "                                series = data[col].dropna()\n",
    "                                if not series.empty:\n",
    "                                    return series\n",
    "                        else:\n",
    "                            return data.iloc[:, 0].dropna()\n",
    "                \n",
    "                # If still no data, try alternative approach\n",
    "                data = blp.bdh(ticker, 'PX_LAST', start_date, end_date)\n",
    "                if not data.empty:\n",
    "                    return data.iloc[:, 0].dropna() if len(data.columns) > 0 else pd.Series()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"      Attempt {attempt + 1} failed for {ticker}: {str(e)[:100]}...\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(1 + attempt)  # Progressive backoff\n",
    "        \n",
    "        print(f\"      Failed to retrieve data for {ticker} after {retries} attempts\")\n",
    "        return pd.Series()\n",
    "    \n",
    "    def _stitch_proxy_to_etf(self, proxy_data, etf_data, splice_date):\n",
    "        \"\"\"\n",
    "        Stitch proxy data to ETF data at splice point with improved scaling\n",
    "        \"\"\"\n",
    "        if proxy_data.empty and etf_data.empty:\n",
    "            return pd.Series()\n",
    "        elif proxy_data.empty:\n",
    "            return etf_data\n",
    "        elif etf_data.empty:\n",
    "            return proxy_data\n",
    "        \n",
    "        splice_date = pd.to_datetime(splice_date)\n",
    "        \n",
    "        # Get pre-splice proxy data\n",
    "        pre_splice = proxy_data[proxy_data.index < splice_date].copy()\n",
    "        \n",
    "        if pre_splice.empty:\n",
    "            return etf_data\n",
    "        \n",
    "        # Find best scaling approach\n",
    "        etf_start_value = etf_data.iloc[0] if not etf_data.empty else None\n",
    "        proxy_end_value = pre_splice.iloc[-1] if not pre_splice.empty else None\n",
    "        \n",
    "        if etf_start_value and proxy_end_value and proxy_end_value != 0:\n",
    "            # Method 1: Direct scaling\n",
    "            scale_factor = etf_start_value / proxy_end_value\n",
    "            scaled_proxy = pre_splice * scale_factor\n",
    "            \n",
    "            # Combine scaled proxy with ETF data\n",
    "            combined = pd.concat([scaled_proxy, etf_data])\n",
    "            return combined.sort_index()\n",
    "        else:\n",
    "            # Method 2: Normalize both series to 100 at splice point\n",
    "            if not pre_splice.empty and not etf_data.empty:\n",
    "                proxy_normalized = (pre_splice / pre_splice.iloc[-1]) * 100\n",
    "                etf_normalized = (etf_data / etf_data.iloc[0]) * 100\n",
    "                \n",
    "                combined = pd.concat([proxy_normalized, etf_normalized])\n",
    "                return combined.sort_index()\n",
    "        \n",
    "        # Fallback: just concatenate\n",
    "        combined = pd.concat([pre_splice, etf_data])\n",
    "        return combined.sort_index()\n",
    "    \n",
    "    def _get_cash_data(self, start_date, end_date, frequency):\n",
    "        \"\"\"\n",
    "        Get cash proxy data with improved yield-to-price conversion and fallback logic\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try primary cash proxy (3-month Treasury)\n",
    "            cash_series = self._fetch_single_series(\n",
    "                self.cash_proxy, start_date, end_date, frequency\n",
    "            )\n",
    "            \n",
    "            if cash_series.empty:\n",
    "                # Try alternative (Federal Funds Rate)\n",
    "                print(\"    Trying alternative cash proxy (Fed Funds)...\")\n",
    "                cash_series = self._fetch_single_series(\n",
    "                    self.cash_alt, start_date, end_date, frequency\n",
    "                )\n",
    "            \n",
    "            if not cash_series.empty:\n",
    "                # Convert yield to cumulative return index\n",
    "                # Assuming yields are in percentage terms\n",
    "                monthly_returns = cash_series / 100 / 12  # Convert annual % to monthly\n",
    "                monthly_returns = monthly_returns.fillna(method='ffill').fillna(0)\n",
    "                \n",
    "                # Build cumulative return index starting at 100\n",
    "                cash_index = (1 + monthly_returns).cumprod() * 100\n",
    "                return cash_index\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error in cash data processing: {e}\")\n",
    "        \n",
    "        # Fallback: create flat cash series at 100\n",
    "        print(\"    Using fallback flat cash series\")\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "        return pd.Series(100.0, index=date_range, name='CASH')\n",
    "    \n",
    "    def _validate_data_quality(self, data):\n",
    "        \"\"\"\n",
    "        Comprehensive data quality validation\n",
    "        \"\"\"\n",
    "        quality_report = {}\n",
    "        \n",
    "        for column in data.columns:\n",
    "            series = data[column]\n",
    "            \n",
    "            quality_report[column] = {\n",
    "                'total_observations': len(series),\n",
    "                'missing_count': series.isnull().sum(),\n",
    "                'missing_percentage': series.isnull().mean() * 100,\n",
    "                'zero_values': (series == 0).sum(),\n",
    "                'negative_values': (series < 0).sum() if series.dtype in ['float64', 'int64'] else 0,\n",
    "                'first_valid_date': series.first_valid_index(),\n",
    "                'last_valid_date': series.last_valid_index(),\n",
    "                'data_range': (series.min(), series.max()) if not series.empty and series.notna().any() else (None, None),\n",
    "                'suspicious_gaps': self._detect_suspicious_gaps(series),\n",
    "                'data_coverage': (len(series) - series.isnull().sum()) / len(series) * 100\n",
    "            }\n",
    "        \n",
    "        return quality_report\n",
    "    \n",
    "    def _detect_suspicious_gaps(self, series, max_gap_days=65):\n",
    "        \"\"\"\n",
    "        Detect suspicious data gaps longer than threshold (adjusted for monthly data)\n",
    "        \"\"\"\n",
    "        if series.empty or series.isnull().all():\n",
    "            return []\n",
    "        \n",
    "        # Find gaps in data\n",
    "        valid_dates = series.dropna().index\n",
    "        if len(valid_dates) < 2:\n",
    "            return []\n",
    "        \n",
    "        gaps = []\n",
    "        for i in range(1, len(valid_dates)):\n",
    "            gap_days = (valid_dates[i] - valid_dates[i-1]).days\n",
    "            if gap_days > max_gap_days:  # More than ~2 months for monthly data\n",
    "                gaps.append({\n",
    "                    'start': valid_dates[i-1],\n",
    "                    'end': valid_dates[i], \n",
    "                    'days': gap_days\n",
    "                })\n",
    "        \n",
    "        return gaps\n",
    "    \n",
    "    def _print_quality_report(self, quality_report):\n",
    "        \"\"\"\n",
    "        Print formatted data quality report\n",
    "        \"\"\"\n",
    "        for asset, metrics in quality_report.items():\n",
    "            coverage = metrics['data_coverage']\n",
    "            if coverage > 90:\n",
    "                status = \"✓\"\n",
    "            elif coverage > 50:\n",
    "                status = \"⚠\"\n",
    "            else:\n",
    "                status = \"✗\"\n",
    "                \n",
    "            print(f\"\\n{status} {asset}:\")\n",
    "            print(f\"  Observations: {metrics['total_observations']}\")\n",
    "            print(f\"  Coverage: {coverage:.1f}% ({metrics['total_observations'] - metrics['missing_count']} valid)\")\n",
    "            \n",
    "            if metrics['first_valid_date'] and metrics['last_valid_date']:\n",
    "                print(f\"  Date Range: {metrics['first_valid_date'].strftime('%Y-%m')} to {metrics['last_valid_date'].strftime('%Y-%m')}\")\n",
    "                \n",
    "                if metrics['data_range'][0] is not None:\n",
    "                    print(f\"  Value Range: {metrics['data_range'][0]:.2f} to {metrics['data_range'][1]:.2f}\")\n",
    "            \n",
    "            if metrics['suspicious_gaps']:\n",
    "                print(f\"  ⚠ Data Gaps: {len(metrics['suspicious_gaps'])} gaps > 65 days\")\n",
    "    \n",
    "    def save_data_to_files(self, data, base_filename='defense_first_data'):\n",
    "        \"\"\"\n",
    "        Save processed data with proper datetime index formatting - Fixed Excel writer\n",
    "        \"\"\"\n",
    "        # Remove completely empty columns\n",
    "        data_clean = data.dropna(axis=1, how='all')\n",
    "        \n",
    "        # Ensure index is properly named and timezone-naive\n",
    "        data_clean.index.name = 'Date'\n",
    "        if hasattr(data_clean.index, 'tz') and data_clean.index.tz is not None:\n",
    "            data_clean.index = data_clean.index.tz_localize(None)\n",
    "        \n",
    "        # CSV format with proper date formatting\n",
    "        data_clean.to_csv(f'{base_filename}.csv', date_format='%Y-%m-%d')\n",
    "        print(f\"Data saved to {base_filename}.csv (Date as datetime index)\")\n",
    "        \n",
    "        # Parquet format preserves datetime index automatically\n",
    "        data_clean.to_parquet(f'{base_filename}.parquet')\n",
    "        print(f\"Data saved to {base_filename}.parquet (datetime index preserved)\")\n",
    "        \n",
    "        # Excel format with multiple sheets - fixed writer\n",
    "        try:\n",
    "            with pd.ExcelWriter(f'{base_filename}.xlsx', engine='openpyxl') as writer:\n",
    "                # Main data\n",
    "                data_clean.to_excel(writer, sheet_name='Price_Data')\n",
    "                \n",
    "                # Quality metrics\n",
    "                quality_df = pd.DataFrame(self._validate_data_quality(data)).T\n",
    "                quality_df.to_excel(writer, sheet_name='Quality_Metrics')\n",
    "                \n",
    "                # Returns calculation\n",
    "                returns = data_clean.pct_change().dropna()\n",
    "                if not returns.empty:\n",
    "                    returns.index.name = 'Date'\n",
    "                    if hasattr(returns.index, 'tz') and returns.index.tz is not None:\n",
    "                        returns.index = returns.index.tz_localize(None)\n",
    "                    returns.to_excel(writer, sheet_name='Monthly_Returns')\n",
    "                \n",
    "                # Summary statistics\n",
    "                summary_stats = data_clean.describe()\n",
    "                summary_stats.to_excel(writer, sheet_name='Summary_Stats')\n",
    "                \n",
    "                # Metadata\n",
    "                metadata = pd.DataFrame({\n",
    "                    'Dataset_Info': [\n",
    "                        f'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}',\n",
    "                        f'Frequency: Monthly',\n",
    "                        f'Start_Date: {data_clean.index.min().strftime(\"%Y-%m-%d\")}',\n",
    "                        f'End_Date: {data_clean.index.max().strftime(\"%Y-%m-%d\")}',\n",
    "                        f'Total_Observations: {len(data_clean)}',\n",
    "                        f'Assets: {\", \".join(data_clean.columns)}',\n",
    "                        'Strategy: Defense First Tactical Allocation'\n",
    "                    ]\n",
    "                })\n",
    "                metadata.to_excel(writer, sheet_name='Metadata', index=False)\n",
    "            \n",
    "            print(f\"Data saved to {base_filename}.xlsx (datetime index with proper formatting)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Excel save failed ({e}), but CSV and Parquet saved successfully\")\n",
    "        \n",
    "        # Create a data loading helper function file\n",
    "        self._create_data_loader_helper(base_filename)\n",
    "\n",
    "    def _create_data_loader_helper(self, base_filename):\n",
    "        \"\"\"\n",
    "        Create a helper Python file for properly loading the saved data\n",
    "        \"\"\"\n",
    "        helper_code = f'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def load_defense_first_data(file_format='parquet', file_path='{base_filename}'):\n",
    "    \"\"\"\n",
    "    Load Defense First dataset with proper datetime index\n",
    "    \n",
    "    Parameters:\n",
    "    file_format: 'csv', 'parquet', or 'excel'\n",
    "    file_path: base filename (without extension)\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with datetime index\n",
    "    \"\"\"\n",
    "    \n",
    "    if file_format.lower() == 'csv':\n",
    "        # Load CSV with proper datetime parsing\n",
    "        data = pd.read_csv(\n",
    "            f'{{file_path}}.csv',\n",
    "            index_col=0,\n",
    "            parse_dates=True\n",
    "        )\n",
    "        \n",
    "    elif file_format.lower() == 'parquet':\n",
    "        # Load Parquet (datetime index preserved automatically)\n",
    "        data = pd.read_parquet(f'{{file_path}}.parquet')\n",
    "        \n",
    "    elif file_format.lower() == 'excel':\n",
    "        # Load Excel with proper datetime parsing\n",
    "        data = pd.read_excel(\n",
    "            f'{{file_path}}.xlsx',\n",
    "            sheet_name='Price_Data',\n",
    "            index_col=0,\n",
    "            parse_dates=True\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"file_format must be 'csv', 'parquet', or 'excel'\")\n",
    "    \n",
    "    # Ensure index is datetime and properly named\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data.index.name = 'Date'\n",
    "    \n",
    "    # Sort by date to ensure proper chronological order\n",
    "    data = data.sort_index()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_returns_data(file_path='{base_filename}'):\n",
    "    \"\"\"Load monthly returns with datetime index\"\"\"\n",
    "    try:\n",
    "        returns = pd.read_excel(\n",
    "            f'{{file_path}}.xlsx',\n",
    "            sheet_name='Monthly_Returns',\n",
    "            index_col=0,\n",
    "            parse_dates=True\n",
    "        )\n",
    "        returns.index = pd.to_datetime(returns.index)\n",
    "        returns.index.name = 'Date'\n",
    "        return returns.sort_index()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load returns from Excel: {{e}}\")\n",
    "        # Calculate returns from main data\n",
    "        data = load_defense_first_data('parquet', file_path)\n",
    "        returns = data.pct_change().dropna()\n",
    "        return returns\n",
    "\n",
    "def get_data_summary(file_path='{base_filename}'):\n",
    "    \"\"\"Get comprehensive data summary\"\"\"\n",
    "    \n",
    "    # Load main data\n",
    "    data = load_defense_first_data('parquet', file_path)\n",
    "    \n",
    "    summary = {{\n",
    "        'shape': data.shape,\n",
    "        'date_range': (data.index.min(), data.index.max()),\n",
    "        'frequency': pd.infer_freq(data.index),\n",
    "        'assets': list(data.columns),\n",
    "        'missing_data': data.isnull().sum().to_dict(),\n",
    "        'data_coverage': ((len(data) - data.isnull().sum()) / len(data) * 100).to_dict()\n",
    "    }}\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    data = load_defense_first_data('parquet')  # Recommended format\n",
    "    print(f\"Loaded data shape: {{data.shape}}\")\n",
    "    print(f\"Date range: {{data.index.min()}} to {{data.index.max()}}\")\n",
    "    print(f\"Assets: {{', '.join(data.columns)}}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\\\nSample data (last 5 observations):\")\n",
    "    print(data.tail())\n",
    "    \n",
    "    # Load returns\n",
    "    returns = load_returns_data()\n",
    "    print(f\"\\\\nReturns data shape: {{returns.shape}}\")\n",
    "    \n",
    "    # Get summary\n",
    "    summary = get_data_summary()\n",
    "    print(f\"\\\\nData Summary:\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"  {{key}}: {{value}}\")\n",
    "'''\n",
    "        \n",
    "        with open(f'{base_filename}_loader.py', 'w') as f:\n",
    "            f.write(helper_code)\n",
    "        \n",
    "        print(f\"Data loader helper saved to {base_filename}_loader.py\")\n",
    "\n",
    "    def validate_datetime_index(self, data):\n",
    "        \"\"\"\n",
    "        Validate that datetime index is properly formatted\n",
    "        \"\"\"\n",
    "        checks = {\n",
    "            'is_datetime_index': isinstance(data.index, pd.DatetimeIndex),\n",
    "            'is_sorted': data.index.is_monotonic_increasing,\n",
    "            'has_duplicates': data.index.duplicated().any(),\n",
    "            'frequency': pd.infer_freq(data.index),\n",
    "            'timezone_aware': data.index.tz is not None,\n",
    "            'index_name': data.index.name\n",
    "        }\n",
    "        \n",
    "        print(\"\\n=== DATETIME INDEX VALIDATION ===\")\n",
    "        for check, result in checks.items():\n",
    "            status = \"✓\" if (result if check != 'has_duplicates' else not result) else \"✗\"\n",
    "            print(f\"{status} {check}: {result}\")\n",
    "        \n",
    "        return all([\n",
    "            checks['is_datetime_index'],\n",
    "            checks['is_sorted'], \n",
    "            not checks['has_duplicates']\n",
    "        ])\n",
    "    \n",
    "    def check_bloomberg_connectivity(self):\n",
    "        \"\"\"\n",
    "        Test Bloomberg connection with xbbg specific tests\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Simple connectivity test\n",
    "            test_data = blp.bdh('SPY US Equity', 'PX_LAST', '2020-01-01', '2020-01-31')\n",
    "            if not test_data.empty:\n",
    "                print(\"✓ Bloomberg xbbg connectivity successful\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"✗ Bloomberg returned empty data\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Bloomberg connectivity issue: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def test_individual_tickers(self):\n",
    "        \"\"\"\n",
    "        Test each ticker individually to identify specific issues\n",
    "        \"\"\"\n",
    "        print(\"\\n=== INDIVIDUAL TICKER TESTING ===\")\n",
    "        \n",
    "        all_tickers = []\n",
    "        for asset_config in self.assets.values():\n",
    "            all_tickers.extend([\n",
    "                asset_config['bloomberg'],\n",
    "                asset_config['proxy_pre_inception'],\n",
    "                asset_config.get('proxy_alt', '')\n",
    "            ])\n",
    "        all_tickers.append(self.cash_proxy)\n",
    "        all_tickers = [t for t in all_tickers if t]  # Remove empty strings\n",
    "        \n",
    "        results = {}\n",
    "        for ticker in set(all_tickers):  # Remove duplicates\n",
    "            try:\n",
    "                test_data = self._fetch_single_series(ticker, '2020-01-01', '2020-03-31', 'M')\n",
    "                if not test_data.empty:\n",
    "                    results[ticker] = f\"✓ {len(test_data)} observations\"\n",
    "                else:\n",
    "                    results[ticker] = \"✗ No data retrieved\"\n",
    "            except Exception as e:\n",
    "                results[ticker] = f\"✗ Error: {str(e)[:50]}\"\n",
    "        \n",
    "        for ticker, result in results.items():\n",
    "            print(f\"  {ticker}: {result}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Updated main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize data pipeline\n",
    "    pipeline = DefenseFirstDataPipeline()\n",
    "    \n",
    "    # Check Bloomberg connectivity\n",
    "    if not pipeline.check_bloomberg_connectivity():\n",
    "        print(\"Bloomberg connection failed - check xbbg setup\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Test individual tickers\n",
    "    pipeline.test_individual_tickers()\n",
    "    \n",
    "    # Get complete dataset\n",
    "    print(\"\\n=== FETCHING COMPLETE DATASET ===\")\n",
    "    data = pipeline.get_complete_dataset(\n",
    "        start_date='1986-01-01', \n",
    "        end_date='2025-06-30',\n",
    "        frequency='M'\n",
    "    )\n",
    "    \n",
    "    if not data.empty and not data.dropna(axis=1, how='all').empty:\n",
    "        clean_data = data.dropna(axis=1, how='all')\n",
    "        \n",
    "        # Validate datetime index\n",
    "        is_valid_index = pipeline.validate_datetime_index(clean_data)\n",
    "        \n",
    "        print(f\"\\nDataset shape: {clean_data.shape}\")\n",
    "        print(f\"Date range: {clean_data.index.min().strftime('%Y-%m-%d')} to {clean_data.index.max().strftime('%Y-%m-%d')}\")\n",
    "        print(f\"Frequency: {pd.infer_freq(clean_data.index)}\")\n",
    "        print(f\"Assets with data: {', '.join(clean_data.columns[clean_data.notna().any()])}\")\n",
    "        \n",
    "        # Save processed data with proper datetime formatting\n",
    "        pipeline.save_data_to_files(clean_data)\n",
    "        \n",
    "        # Display sample data with formatted dates\n",
    "        print(\"\\n=== SAMPLE DATA (Last 5 observations) ===\")\n",
    "        sample_data = clean_data.tail()\n",
    "        sample_data.index = sample_data.index.strftime('%Y-%m-%d')\n",
    "        print(sample_data)\n",
    "        \n",
    "        # Display first valid data points with formatted dates\n",
    "        print(\"\\n=== FIRST VALID DATA POINTS ===\")\n",
    "        for col in clean_data.columns:\n",
    "            first_valid = clean_data[col].first_valid_index()\n",
    "            if first_valid:\n",
    "                first_val = clean_data.loc[first_valid, col]\n",
    "                print(f\"{col}: {first_valid.strftime('%Y-%m-%d')} = {first_val:.2f}\")\n",
    "        \n",
    "        print(f\"\\n✓ All files saved with proper datetime index formatting\")\n",
    "        print(f\"✓ Data loader helper created: defense_first_data_loader.py\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to retrieve any valid data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d39fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-ml-GtY3jPFV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
